{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8618955-56f8-4c7c-a485-1be205eec11c",
   "metadata": {},
   "source": [
    "# [`nn.modules.linear.Linear`](https://github.com/JamorMoussa/NanoTorch/blob/4092b0fe7cd19cca1db2a3c99b6f5b77af9dc8a6/nanotorch/nn/modules/linear.py#L9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121a7069-4bf6-46fb-8a0b-67fe921f8541",
   "metadata": {},
   "source": [
    "In this document, we're going to cover the **Linear** layer, also called **Dense** layer, from the theory to an efficient implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ceb1e9-57a0-4924-919f-5380bf2397ce",
   "metadata": {},
   "source": [
    "In the first place, we're going to cover how a single neuron works, we extend this concept to build **fully connected Layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1956d7c-1118-4f99-a436-044791f9fac5",
   "metadata": {},
   "source": [
    "## 01. Theory - Build Fully Connected Layer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482398e4-b00f-48a3-b404-a7d0cf42ffed",
   "metadata": {},
   "source": [
    "The fully connected layer is a fundamental building block of neural networks. It performs a linear transformation on the input, where each node is fully connected to every node in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab2b6d-3fca-4a66-9fe5-27f034fa0e0e",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/3_fully-connected-layer_0.jpg\" width=\"500\" />\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa66e43-a646-4038-b3b6-e9237b44e1f3",
   "metadata": {},
   "source": [
    "To simplify this concept, we'll first explore how a single neuron works. Once we understand this, we can extend the idea to build a fully connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe5951-4f0c-4ec6-b248-aa91f53572e5",
   "metadata": {},
   "source": [
    "### 1.1 Neural Nets - Artificial Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ca841-ed62-4401-a5ab-152fdd0b9fc0",
   "metadata": {},
   "source": [
    "The **Artificial Neuron** is the basic unit used to build more complex neural networks. In this section, we'll delve into the mathematical workings of this neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f72e1-a4ee-44f6-a83a-76baed613185",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://raw.githubusercontent.com/JamorMoussa/NanoTorch/dev/docs/images/docs/linear/neuron.png\" width=\"400\" />\n",
    "        <figcaption> <b>Artificial Neuron</figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46620ad-a87d-41a2-bebf-f73d8fde63ad",
   "metadata": {},
   "source": [
    "The **Artificial Neuron** is a processing unit that takes some given input and produces an output.\n",
    "\n",
    "Mathematically, it can be described as a **function** that accepts an **input vector** $x \\in \\mathbb{R}^n$ and returns a **weighted sum** of that input with a **weight vector** $w \\in \\mathbb{R}^n$, which has the same dimension as the input $x$, and then adds a **bias** $b \\in \\mathbb{R}$, finally returning a scalar output $y \\in \\mathbb{R}$.\n",
    "\n",
    "Formally,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y = w_1 x_1 + w_2 x_2 + \\hspace{0.2cm} \\dots \\hspace{0.2cm} + w_n x_n + b &= \\sum_{i = 1}^{n} w_i x_i + b \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f52852c-b651-4246-8555-63c531b32178",
   "metadata": {},
   "source": [
    "The weight $w_i$ describes the importance of the corresponding feature $x_i$, indicating how much it contributes to computing the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda209e-6a5c-4e44-898a-470154677398",
   "metadata": {},
   "source": [
    "The weight vector $w$ and bias $b$ are called learnable parameters, meaning they are learned during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e466cf-7e60-43af-84b0-4941e2ca6002",
   "metadata": {},
   "source": [
    "Acually, we can add the bias $b$ in the weighted sum, by consedering the $w_0 = b$ and set the $x_0 = 1$.\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y &= \\sum_{i = 1}^{n} w_i x_i + b = \\sum_{i = 1}^{n} w_i x_i + w_0 x_0 = \\sum_{i = 0}^{n} w_i x_i \n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408833ba-3fdf-46e2-b033-39acec3c09ae",
   "metadata": {},
   "source": [
    "There is another way to compute the output $y$ using the dot product of the input vector $x = (1, x_{org}) \\in \\mathbb{R}^{n+1}$ and the weight vector $w = (b, w_{org}) \\in \\mathbb{R}^{n+1}$ as follows:\n",
    "\n",
    "$$\n",
    "y = w^T x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8db945-4d68-4d22-8056-66744a3d47fa",
   "metadata": {},
   "source": [
    "### 1.2 Fully Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90c3b4-3028-41b0-8614-c2092fec7f37",
   "metadata": {},
   "source": [
    "\"In the previous section, we saw how a single artificial neuron operates. Now, we can map the same input vector $x \\in \\mathbb{R}^{n}$ to multiple neurons and perform the same operation as before. This creates a structure called a **Fully Connected Layer**, where all output nodes are fully connected to the input nodes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8a36d-cab7-4dc9-bb1a-1f8fd9bc2992",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://raw.githubusercontent.com/JamorMoussa/NanoTorch/dev/docs/images/docs/linear/fully-connected-layer.png\" width=\"400\" />\n",
    "        <figcaption> <b>Fully Connected Layer</figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d408e66-2228-4730-a8c6-b9079e32cca2",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\" markdown=\"\">\n",
    "<p class=\"admonition-title\">Note</p>\n",
    "<p> We will adopt a notation to maintain consistency in writing equations where the weight connecting input node $i$ to output node $j$ is denoted as $w_{ij}$. </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd98e93-fa5d-4396-b3e5-bc5145be7b24",
   "metadata": {},
   "source": [
    "Let's start with the first output, considering it as a single neuron performing the same computation as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e0d49-0c07-418f-b30d-eb455a43a9fb",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://raw.githubusercontent.com/JamorMoussa/NanoTorch/dev/docs/images/docs/linear/fully-connected-layer-1.png\" width=\"400\" />\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758b9fe-9de3-462a-98f0-858694534cd4",
   "metadata": {},
   "source": [
    "$$\n",
    "    y_{1} = w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + \\hspace{0.2cm} \\dots \\hspace{0.2cm} + w_{1n}x_n + w_{10} = \\sum_{i=1}^{n}w_{1i}x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64dc4f8-db76-4d6a-9059-3b50d0d3bf11",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://raw.githubusercontent.com/JamorMoussa/NanoTorch/dev/docs/images/docs/linear/fully-connected-layer-2.png\" width=\"400\" />\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad7d50-8012-45a2-abc1-9e147785526d",
   "metadata": {},
   "source": [
    "$$\n",
    "    y_{2} = w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + \\hspace{0.2cm} \\dots \\hspace{0.2cm} + w_{2n}x_n + w_{20} = \\sum_{i=1}^{n}w_{2i}x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbe189-86f9-4f99-a1fd-439de4478983",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://raw.githubusercontent.com/JamorMoussa/NanoTorch/dev/docs/images/docs/linear/fully-connected-layer-3.png\" width=\"400\" />\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d35efc-cd59-409b-93b2-ca6b6b66bcc6",
   "metadata": {},
   "source": [
    "$$\n",
    "    y_{m} = w_{m1}x_1 + w_{m2}x_2 + w_{m3}x_3 + \\hspace{0.2cm} \\dots \\hspace{0.2cm} + w_{mn}x_n + w_{m0} = \\sum_{i=1}^{n}w_{mi}x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369eeecb-7a77-427f-bdd8-4a0dcfdbb3d0",
   "metadata": {},
   "source": [
    "Beautiful. Let's stack all the equations into a single system of linear equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4abb5b1-2c3a-4cd2-a1b7-b89dfd7a6284",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "     &y_{1} = w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + \\dots + w_{1n}x_n + w_{10} \\\\\n",
    "     &y_{2} = w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + \\dots + w_{2n}x_n + w_{20} \\\\\n",
    "     &\\vdots \\\\\n",
    "     &y_{m} = w_{m1}x_1 + w_{m2}x_2 + w_{m3}x_3 + \\dots + w_{mn}x_n + w_{m0} \\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Hey, does this remind you of something, a pattern here?\n",
    "\n",
    "Let's turn this system of linear equations into matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c012e89-9a47-4d43-b9c0-2a6b0d2b28e0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_m\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "w_{10} & w_{11} & w_{12} & \\dots & w_{1n} \\\\\n",
    "w_{20} & w_{21} & w_{22} & \\dots & w_{2n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{m0} & w_{m1} & w_{m2} & \\dots & w_{mn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7a13a4-a019-40b1-91f9-40e585dc278a",
   "metadata": {},
   "source": [
    "Thus, we can use the matrix formula to describe the computation of a fully connected layer as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{y} &= W \\mathbf{x}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "W  = \\begin{bmatrix}\n",
    "w_{10} & w_{11} & w_{12} & \\dots & w_{1n} \\\\\n",
    "w_{20} & w_{21} & w_{22} & \\dots & w_{2n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{m0} & w_{m1} & w_{m2} & \\dots & w_{mn}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m \\times (n+1)}\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{x} \\in \\mathbb{R}^{(n+1)}$ and $\\mathbf{y} \\in \\mathbb{R}^{m}$ denote the input and output vectors of the fully connected layer, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a0aa3-188f-40d6-9e15-62acd37ece6d",
   "metadata": {},
   "source": [
    "### 1.4 Forward Propopagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e37d6-aa59-405b-bb15-269cff214a90",
   "metadata": {},
   "source": [
    "In the previous section, we demonstrated that we could construct a fully connected layer with any number of inputs, denoted as `in_features`, and produce any number of outputs, denoted as `out_features`, by constructing a learnable matrix $W$ with dimensions *in\\_features* $\\times$ *out\\_features*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4391bf-05c7-4b77-a144-5594d2ff0e50",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://raw.githubusercontent.com/JamorMoussa/NanoTorch/dev/docs/images/docs/linear/fully-connect-layer-forward-pass.png\" width=\"300\" />\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f6542e-2b68-4f4c-a4c2-34be7928e6aa",
   "metadata": {},
   "source": [
    "The forward pass is performed when we compute the output, given a input vector $x \\in \\mathbb{R}^{(\\text{in\\_features} + 1)}$ : \n",
    "\n",
    "$$\n",
    "    \\mathbf{y} = W \\mathbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbc340-63fd-4eff-a710-518a15b2cea0",
   "metadata": {},
   "source": [
    "### 1.5 Back-Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e69fb-d779-490e-939f-fe75251936b0",
   "metadata": {},
   "source": [
    "This is the most exciting part.\n",
    "\n",
    "The whole point of machine learning is to train algorithms. The process of training involves evaluating a loss function (depending on the specific task), computing the gradients of this loss with respect to the model's parameters $W$, and then using any optimization methods, such as **Adam**, to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd25a7-3fdf-47d6-8061-b7ac1e804872",
   "metadata": {},
   "source": [
    "Let's denote the loss function used to evaluate the model's performance as $L$.\n",
    "\n",
    "The following figure shows that the fully connected layer receives the gradient flows from the subsequent layer, denoted as $\\frac{\\partial L}{\\partial \\mathbf{y}}$. This quantity is used to compute the gradient of the loss with respect to the current layer's parameters $\\frac{\\partial L}{\\partial W}$. Then, it passes the gradient with respect to the input to the previous layers $\\frac{\\partial L}{\\partial \\mathbf{x}}$, following the chain rule in backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72511b7-8639-40e8-892a-6f7128e9a0ac",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://raw.githubusercontent.com/JamorMoussa/NanoTorch/dev/docs/images/docs/linear/linear-back-propagation.png\" width=\"300\" />\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f345b-75a9-401d-a44d-afeff9063409",
   "metadata": {},
   "source": [
    "For instance, let's break down each derivative.\n",
    "\n",
    "The loss function is a scalar value, i.e., $L \\in \\mathbb{R}$. Let $\\mathbf{v}$ be a vector of n-dimensions, i.e., $\\mathbf{v} \\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a060bab-59e8-440c-afd1-fccd06f03f33",
   "metadata": {},
   "source": [
    "So, the derivative of $L$ with respect to $\\mathbf{v}$ is defined as the derivative of $L$ for each component of $\\mathbf{v}$. Formally:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{v}} = \\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial v_1} \\\\\n",
    "\\frac{\\partial L}{\\partial v_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial v_n}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d679d2a-2169-456f-bf82-c27a0535915f",
   "metadata": {},
   "source": [
    "With the same logic, given a matrix $M \\in \\mathbb{R}^{m \\times n}$, the derivative of $L$ with respect to $M$ is defined as the derivative of $L$ for each component of $M$. Formally:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial M} = \\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial M_{11}} & \\frac{\\partial L}{\\partial M_{12}} & \\cdots & \\frac{\\partial L}{\\partial M_{1n}} \\\\\n",
    "\\frac{\\partial L}{\\partial M_{21}} & \\frac{\\partial L}{\\partial M_{22}} & \\cdots & \\frac{\\partial L}{\\partial M_{2n}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial M_{m1}} & \\frac{\\partial L}{\\partial M_{m2}} & \\cdots & \\frac{\\partial L}{\\partial M_{mn}}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929aa553-91eb-4bef-9bf0-611225ae0d54",
   "metadata": {},
   "source": [
    "#### 1.5.1 Compute $\\frac{\\partial L}{\\partial W}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b80af5-150d-4db8-b745-28e40281e944",
   "metadata": {},
   "source": [
    "Since our layer receives the quantity $\\frac{\\partial L}{\\partial \\mathbf{y}}$ during back-propagation, our task is to use it to compute the derivative of $L$ with respect to $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34173b9-ab3c-4742-8aad-8d15485e56b0",
   "metadata": {},
   "source": [
    "Given row index $i \\in \\{1, ..., n\\}$ and column index $j \\in \\{1, ..., m\\}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{ij}} = \\frac{\\partial L}{\\partial y_1} \\underbrace{\\frac{\\partial y_1}{\\partial W_{ij}}}_{=0} + \\frac{\\partial L}{\\partial y_2} \\underbrace{\\frac{\\partial y_2}{\\partial W_{ij}}}_{=0} + \\dots + \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial W_{ij}} + \\dots + \\frac{\\partial L}{\\partial y_n} \\underbrace{\\frac{\\partial y_n}{\\partial W_{ij}}}_{=0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1debe03f-2623-49ed-b2b4-b0dea0bbb46f",
   "metadata": {},
   "source": [
    "Thus,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{ij}} = \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial W_{ij}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f801e-b8dc-4966-b0c3-6fc48be804f0",
   "metadata": {},
   "source": [
    "We have:\n",
    "\n",
    "$$\n",
    "y_i = W_{i1}x_1 + \\dots + W_{ij}x_j + \\dots + W_{im}x_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0edccc-9691-4669-a4f6-6f5d123270c8",
   "metadata": {},
   "source": [
    "Then, the derivative of $y_i$ with respect to $W_{ij}$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial W_{ij}} = x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb5624-f7ee-4400-abaa-40ea64531aba",
   "metadata": {},
   "source": [
    "Finally, \n",
    "\n",
    "$$\n",
    "    \\forall i \\in  \\{1, ..., n \\}, j \\in \\{1, ..., m\\} \\mid \\frac{\\partial L}{\\partial W_{ij}} = \\frac{\\partial L}{\\partial y_i} x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df1afc-ce5a-4113-bb5c-022b8b38d0a1",
   "metadata": {},
   "source": [
    "Using this formula to fill the matrix $\\frac{\\partial L}{\\partial W}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial W_{11}} & \\frac{\\partial L}{\\partial W_{12}} & \\cdots & \\frac{\\partial L}{\\partial W_{1n}} \\\\\n",
    "\\frac{\\partial L}{\\partial W_{21}} & \\frac{\\partial L}{\\partial W_{22}} & \\cdots & \\frac{\\partial L}{\\partial W_{2n}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial W_{m1}} & \\frac{\\partial L}{\\partial W_{m2}} & \\cdots & \\frac{\\partial L}{\\partial W_{mn}}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial y_1} x_1 & \\frac{\\partial L}{\\partial y_1} x_2 & \\cdots &\\frac{\\partial L}{\\partial y_1} x_n \\\\\n",
    "\\frac{\\partial L}{\\partial y_2} x_1 & \\frac{\\partial L}{\\partial y_2} x_2 & \\cdots &\\frac{\\partial L}{\\partial y_2} x_n \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial y_m} x_1 & \\frac{\\partial L}{\\partial y_m} x_2 & \\cdots &\\frac{\\partial L}{\\partial y_m} x_n \\\\\n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial L}{\\partial y_1} \\\\\n",
    "    \\frac{\\partial L}{\\partial y_2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial L}{\\partial y_m}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x_1 & x_2 & \\dots & x_n \\\\\n",
    "\\end{bmatrix} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{x}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cfd831-3df1-4302-8a0a-84bb83beb60f",
   "metadata": {},
   "source": [
    "Finally,\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial W} =  \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{x}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec7f99-3f41-4b66-ac43-e2cdebab0d51",
   "metadata": {},
   "source": [
    "#### 1.5.2 Compute $\\frac{\\partial L}{\\partial \\mathbf{x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf39607-1ad2-42f7-a0c9-0a42aad038e9",
   "metadata": {},
   "source": [
    "With the same logic as before, let's compute the derivative of $L$ with respect to input vector $\\mathbf{x}, i.e. $$\\frac{\\partial L}{\\partial \\mathbf{x}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a44656-993b-4fd5-a046-1bde6f823e56",
   "metadata": {},
   "source": [
    "For given $i \\in {1, ..., n}$ \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial y_1}  \\underbrace{\\frac{\\partial y_1}{\\partial x_i}}_{W_{j1}} + \\dots + \\frac{\\partial L}{\\partial y_j}  \\underbrace{\\frac{\\partial y_j}{\\partial x_i}}_{W_{ji}} + \\dots + \\frac{\\partial L}{\\partial y_m}  \\underbrace{\\frac{\\partial y_m}{\\partial x_i}}_{W_{jm}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa24d2-4045-4a01-b15a-e5838c45e2d0",
   "metadata": {},
   "source": [
    "Because we have:\n",
    "\n",
    "$$\n",
    "y_j = W_{j1}x_1 + \\dots + W_{ji}x_i + \\dots + W_{jm}x_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c858a466-0088-4912-b509-b6aa89fa7952",
   "metadata": {},
   "source": [
    "Thus, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial y_1}W_{j1} + \\dots + \\frac{\\partial L}{\\partial y_j}W_{ji} + \\dots + \\frac{\\partial L}{\\partial y_m} W_{jm}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe997753-95af-4976-9646-c020a9532e3e",
   "metadata": {},
   "source": [
    "Using this formula to fill the vector $\\frac{\\partial L}{\\partial \\mathbf{x}}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{x}} = \\begin{bmatrix}\n",
    "    \\frac{\\partial L}{\\partial x_1} \\\\\n",
    "    \\frac{\\partial L}{\\partial x_2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial L}{\\partial x_n}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{\\partial L}{\\partial y_1}W_{11} + \\dots + \\frac{\\partial L}{\\partial y_j}W_{j1} + \\dots + \\frac{\\partial L}{\\partial y_m} W_{m1} \\\\\n",
    "    \\frac{\\partial L}{\\partial y_1}W_{12} + \\dots + \\frac{\\partial L}{\\partial y_j}W_{j2} + \\dots + \\frac{\\partial L}{\\partial y_m} W_{m2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial L}{\\partial y_1}W_{1n} + \\dots + \\frac{\\partial L}{\\partial y_j}W_{jn} + \\dots + \\frac{\\partial L}{\\partial y_m} W_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    W_{11} & W_{21} & \\dots & W_{m1} \\\\\n",
    "    W_{12} & W_{22} & \\dots & W_{m2} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    W_{1n} & W_{2n} & \\dots & W_{mn}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "    \\frac{\\partial L}{\\partial y_1} \\\\\n",
    "    \\frac{\\partial L}{\\partial y_2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial L}{\\partial y_m}\n",
    "\\end{bmatrix} =  W^T \\frac{\\partial L}{\\partial \\mathbf{y}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c42f8-80dc-4a16-9efb-4a5c201f7c38",
   "metadata": {},
   "source": [
    "Finally,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{x}} = W^T \\frac{\\partial L}{\\partial \\mathbf{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be53d3-860a-4f70-8ec2-899d5c0210c1",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\" markdown=\"\">\n",
    "<p class=\"admonition-title\">Rules to Compute Gradients</p>\n",
    "<p>The layer receives the gradient flow $\\frac{\\partial L}{\\partial \\mathbf{y}}$. Therefore, the gradients can be computed as follows:</p>\n",
    "<p>\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{x}^T\n",
    "$$\n",
    "</p>\n",
    "<p>\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial \\mathbf{x}} = W^T \\frac{\\partial L}{\\partial \\mathbf{y}}\n",
    "$$\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db454d75-1697-49f4-9e45-ad84545a149b",
   "metadata": {},
   "source": [
    "## 02. Implementation - Build Fully Connected Layer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30936b63-9f00-42ce-a597-7e0cba4fc0d5",
   "metadata": {},
   "source": [
    "At this stage we've covered all we need to implement the fully connected layer using only **Numpy**.\n",
    "\n",
    "For instance, we have two pass modes. First, the forward pass performers of computing the output. Second, the backward pass where the gradients are calculated, helps us update the model's parameters, to make more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed00c8d-9f14-4b7a-99aa-fd06dd3e5464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
