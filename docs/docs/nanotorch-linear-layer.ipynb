{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8618955-56f8-4c7c-a485-1be205eec11c",
   "metadata": {},
   "source": [
    "# [`nn.modules.linear.Linear`](https://github.com/JamorMoussa/NanoTorch/blob/4092b0fe7cd19cca1db2a3c99b6f5b77af9dc8a6/nanotorch/nn/modules/linear.py#L9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121a7069-4bf6-46fb-8a0b-67fe921f8541",
   "metadata": {},
   "source": [
    "In this document, we're going to cover the **Linear** layer, also called **Dense** layer, from the theory to an efficient implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ceb1e9-57a0-4924-919f-5380bf2397ce",
   "metadata": {},
   "source": [
    "In the first place, we're going to cover how a single neuron works, we extend this concept to build **fully connected Layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1956d7c-1118-4f99-a436-044791f9fac5",
   "metadata": {},
   "source": [
    "## 01. Theory - Build Fully Connected Layer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482398e4-b00f-48a3-b404-a7d0cf42ffed",
   "metadata": {},
   "source": [
    "The fully connected layer is a fundamental building block of neural networks. It performs a linear transformation on the input, where each node is fully connected to every node in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab2b6d-3fca-4a66-9fe5-27f034fa0e0e",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/3_fully-connected-layer_0.jpg\" width=\"500\" />\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa66e43-a646-4038-b3b6-e9237b44e1f3",
   "metadata": {},
   "source": [
    "To simplify this concept, we'll first explore how a single neuron works. Once we understand this, we can extend the idea to build a fully connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe5951-4f0c-4ec6-b248-aa91f53572e5",
   "metadata": {},
   "source": [
    "### 1.1 Neural Nets - Artificial Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ca841-ed62-4401-a5ab-152fdd0b9fc0",
   "metadata": {},
   "source": [
    "The **Artificial Neuron** is the basic unit used to build more complex neural networks. In this section, we'll delve into the mathematical workings of this neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f72e1-a4ee-44f6-a83a-76baed613185",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://raw.githubusercontent.com/JamorMoussa/NanoTorch/dev/docs/images/docs/linear/neuron.png\" width=\"400\" />\n",
    "        <figcaption> <b>Artificial Neuron</figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46620ad-a87d-41a2-bebf-f73d8fde63ad",
   "metadata": {},
   "source": [
    "The **Artificial Neuron** is a processing unit that takes some given input and produces an output.\n",
    "\n",
    "Mathematically, it can be described as a **function** that accepts an **input vector** $x \\in \\mathbb{R}^n$ and returns a **weighted sum** of that input with a **weight vector** $w \\in \\mathbb{R}^n$, which has the same dimension as the input $x$, and then adds a **bias** $b \\in \\mathbb{R}$, finally returning a scalar output $y \\in \\mathbb{R}$.\n",
    "\n",
    "Formally,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y = w_1 x_1 + w_2 x_2 + \\hspace{0.2cm} \\dots \\hspace{0.2cm} + w_n x_n + b &= \\sum_{i = 1}^{n} w_i x_i + b \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f52852c-b651-4246-8555-63c531b32178",
   "metadata": {},
   "source": [
    "The weight $w_i$ describes the importance of the corresponding feature $x_i$, indicating how much it contributes to computing the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda209e-6a5c-4e44-898a-470154677398",
   "metadata": {},
   "source": [
    "The weight vector $w$ and bias $b$ are called learnable parameters, meaning they are learned during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e466cf-7e60-43af-84b0-4941e2ca6002",
   "metadata": {},
   "source": [
    "Acually, we can add the bias $b$ in the weighted sum, by consedering the $w_0 = b$ and set the $x_0 = 1$.\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y &= \\sum_{i = 1}^{n} w_i x_i + b = \\sum_{i = 1}^{n} w_i x_i + w_0 x_0 = \\sum_{i = 0}^{n} w_i x_i \n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408833ba-3fdf-46e2-b033-39acec3c09ae",
   "metadata": {},
   "source": [
    "There is another way to compute the output $y$ using the dot product of the input vector $x = (1, x_{org}) \\in \\mathbb{R}^{n+1}$ and the weight vector $w = (b, w_{org}) \\in \\mathbb{R}^{n+1}$ as follows:\n",
    "\n",
    "$$\n",
    "y = w^T x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8db945-4d68-4d22-8056-66744a3d47fa",
   "metadata": {},
   "source": [
    "### 1.2 Fully Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90c3b4-3028-41b0-8614-c2092fec7f37",
   "metadata": {},
   "source": [
    "\"In the previous section, we saw how a single artificial neuron operates. Now, we can map the same input vector $x \\in \\mathbb{R}^{n}$ to multiple neurons and perform the same operation as before. This creates a structure called a **Fully Connected Layer**, where all output nodes are fully connected to the input nodes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8a36d-cab7-4dc9-bb1a-1f8fd9bc2992",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://raw.githubusercontent.com/JamorMoussa/NanoTorch/dev/docs/images/docs/linear/fully-connected-layer.png\" width=\"400\" />\n",
    "        <figcaption> <b>Fully Connected Layer</figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d408e66-2228-4730-a8c6-b9079e32cca2",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\" markdown=\"\">\n",
    "<p class=\"admonition-title\">Note</p>\n",
    "<p> We will adopt a notation to maintain consistency in writing equations where the weight connecting input node $i$ to output node $j$ is denoted as $w_{ij}$. </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd98e93-fa5d-4396-b3e5-bc5145be7b24",
   "metadata": {},
   "source": [
    "Let's start with the first output, considering it as a single neuron performing the same computation as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e0d49-0c07-418f-b30d-eb455a43a9fb",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://raw.githubusercontent.com/JamorMoussa/NanoTorch/dev/docs/images/docs/linear/fully-connected-layer-1.png\" width=\"400\" />\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758b9fe-9de3-462a-98f0-858694534cd4",
   "metadata": {},
   "source": [
    "$$\n",
    "    y_{1} = w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + \\hspace{0.2cm} \\dots \\hspace{0.2cm} + w_{1n}x_n + w_{10} = \\sum_{i=1}^{n}w_{1i}x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64dc4f8-db76-4d6a-9059-3b50d0d3bf11",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://raw.githubusercontent.com/JamorMoussa/NanoTorch/dev/docs/images/docs/linear/fully-connected-layer-2.png\" width=\"400\" />\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad7d50-8012-45a2-abc1-9e147785526d",
   "metadata": {},
   "source": [
    "$$\n",
    "    y_{2} = w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + \\hspace{0.2cm} \\dots \\hspace{0.2cm} + w_{2n}x_n + w_{20} = \\sum_{i=1}^{n}w_{2i}x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbe189-86f9-4f99-a1fd-439de4478983",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://raw.githubusercontent.com/JamorMoussa/NanoTorch/dev/docs/images/docs/linear/fully-connected-layer-3.png\" width=\"400\" />\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d35efc-cd59-409b-93b2-ca6b6b66bcc6",
   "metadata": {},
   "source": [
    "$$\n",
    "    y_{m} = w_{m1}x_1 + w_{m2}x_2 + w_{m3}x_3 + \\hspace{0.2cm} \\dots \\hspace{0.2cm} + w_{mn}x_n + w_{m0} = \\sum_{i=1}^{n}w_{mi}x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369eeecb-7a77-427f-bdd8-4a0dcfdbb3d0",
   "metadata": {},
   "source": [
    "Beautiful. Let's stack all the equations into a single system of linear equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4abb5b1-2c3a-4cd2-a1b7-b89dfd7a6284",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "     &y_{1} = w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + \\dots + w_{1n}x_n + w_{10} \\\\\n",
    "     &y_{2} = w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + \\dots + w_{2n}x_n + w_{20} \\\\\n",
    "     &\\vdots \\\\\n",
    "     &y_{m} = w_{m1}x_1 + w_{m2}x_2 + w_{m3}x_3 + \\dots + w_{mn}x_n + w_{m0} \\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Hey, does this remind you of something, a pattern here?\n",
    "\n",
    "Let's turn this system of linear equations into matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c012e89-9a47-4d43-b9c0-2a6b0d2b28e0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_m\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "w_{10} & w_{11} & w_{12} & \\dots & w_{1n} \\\\\n",
    "w_{20} & w_{21} & w_{22} & \\dots & w_{2n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{m0} & w_{m1} & w_{m2} & \\dots & w_{mn}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7a13a4-a019-40b1-91f9-40e585dc278a",
   "metadata": {},
   "source": [
    "Thus, we can use the matrix formula to describe the computation of a fully connected layer as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{y} &= W \\mathbf{x}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "W  = \\begin{pmatrix}\n",
    "w_{10} & w_{11} & w_{12} & \\dots & w_{1n} \\\\\n",
    "w_{20} & w_{21} & w_{22} & \\dots & w_{2n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{m0} & w_{m1} & w_{m2} & \\dots & w_{mn}\n",
    "\\end{pmatrix} \\in \\mathbb{R}^{m \\times (n+1)}\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{x} \\in \\mathbb{R}^{(n+1)}$ and $\\mathbf{y} \\in \\mathbb{R}^{m}$ denote the input and output vectors of the fully connected layer, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a0aa3-188f-40d6-9e15-62acd37ece6d",
   "metadata": {},
   "source": [
    "### 1.4 Forward Propopagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e37d6-aa59-405b-bb15-269cff214a90",
   "metadata": {},
   "source": [
    "In the previous section, we demonstrated that we could construct a fully connected layer with any number of inputs, denoted as `in_features`, and produce any number of outputs, denoted as `out_features`, by constructing a learnable matrix $W$ with dimensions *in\\_features* $\\times$ *out\\_features*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4391bf-05c7-4b77-a144-5594d2ff0e50",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "        <img src=\"https://raw.githubusercontent.com/JamorMoussa/NanoTorch/dev/docs/images/docs/linear/fully-connect-layer-forward-pass.png\" width=\"300\" />\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f6542e-2b68-4f4c-a4c2-34be7928e6aa",
   "metadata": {},
   "source": [
    "The forward pass is performed when we compute the output, given a input vector $x \\in \\mathbb{R}^{(\\text{in\\_features} + 1)}$ : \n",
    "\n",
    "$$\n",
    "    \\mathbf{y} = W \\mathbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbc340-63fd-4eff-a710-518a15b2cea0",
   "metadata": {},
   "source": [
    "### 1.5 Back-Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e69fb-d779-490e-939f-fe75251936b0",
   "metadata": {},
   "source": [
    "This is the most exciting part.\n",
    "\n",
    "The whole point of machine learning is to train algorithms. The process of training involves evaluating a loss function (depending on the specific task), computing the gradients of this loss with respect to the model's parameters $W$, and then using any optimization methods, such as **Adam**, to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd25a7-3fdf-47d6-8061-b7ac1e804872",
   "metadata": {},
   "source": [
    "Let's denote the loss function used to evaluate the model's performance as $L$.\n",
    "\n",
    "The following figure shows that the fully connected layer receives the gradient flows from the subsequent layer, denoted as $\\frac{\\partial L}{\\partial \\mathbf{y}}$. This quantity is used to compute the gradient of the loss with respect to the current layer's parameters $\\frac{\\partial L}{\\partial W}$. Then, it passes the gradient with respect to the input to the previous layers $\\frac{\\partial L}{\\partial \\mathbf{x}}$, following the chain rule in backpropagation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
