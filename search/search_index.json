{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NanoTorch","text":""},{"location":"#what-is-nanotorch","title":"What is NanoTorch?","text":"<p>NanoTorch is a deep learning library inspired by the PyTorch framework, created using only Math and Numpy. It is designed for academic purposes, providing an educational resource for understanding the inner workings of neural networks.</p>"},{"location":"#why-do-we-need-nanotorch","title":"Why Do We Need NanoTorch?","text":"<p>NanoTorch is created for educational purposes, aiming to demystify the complexities of neural networks and their efficient implementation.</p>"},{"location":"#the-purpose-of-nanotorch","title":"The Purpose of NanoTorch","text":"<p>When learning about deep learning, we often encounter Neural Networks\u2014mathematical abstractions of the human brain composed of linear layers and activation functions (non-linearities). These networks learn complex mathematical functions by analyzing large datasets.</p> <p></p> <p>To build these networks, we typically use well-designed frameworks like PyTorch from Meta or TensorFlow from Google. However, several questions arise:</p> <ol> <li>How do these neural networks work behind the scenes?</li> <li>How do frameworks implement them so efficiently?</li> </ol>"},{"location":"#understanding-the-implementation","title":"Understanding the Implementation","text":"<p>While books explain the mathematics behind neural networks, their implementations often fall short of the efficiency and scalability seen in major frameworks. Understanding the source code of frameworks like PyTorch is challenging due to the additional layers of error handling and optimizations.</p>"},{"location":"#the-vision-of-nanotorch","title":"The Vision of NanoTorch","text":"<p>NanoTorch addresses this gap by providing a library with a PyTorch-like syntax and comprehensive documentation. This documentation not only explains the code but also delves into the mathematical concepts (e.g., gradients, matrices) and the process of transforming these equations into efficient and optimized code.</p>"},{"location":"#when-we-use-nanotorch","title":"When we use NanoTorch","text":"<p>Note</p> <p>NanoTorch is not recommended for real-world projects. It is not as powerful as PyTorch or TensorFlow and is intended solely for educational purposes.</p> <p>By understanding how NanoTorch works, you will gain familiarity with the implementation principles of frameworks like PyTorch, providing a strong foundation for further exploration in the field of deep learning.</p>"},{"location":"docs/nanotorch-linear-layer/","title":"","text":"<p>In this document, we're going to cover the Linear layer, also called Dense layer, from the theory to an efficient implementation.</p> <p>In the first place, we're going to cover how a single neuron works, we extend this concept to build fully connected Layer.</p> <p>The fully connected layer is a fundamental building block of neural networks. It performs a linear transformation on the input, where each node is fully connected to every node in the previous layer.</p> <p>To simplify this concept, we'll first explore how a single neuron works. Once we understand this, we can extend the idea to build a fully connected layer.</p> <p>The Artificial Neuron is the basic unit used to build more complex neural networks. In this section, we'll delve into the mathematical workings of this neuron.</p> Artificial Neuron <p>The Artificial Neuron is a processing unit that takes some given input and produces an output.</p> <p>Mathematically, it can be described as a function that accepts an input vector $x \\in \\mathbb{R}^n$ and returns a weighted sum of that input with a weight vector $w \\in \\mathbb{R}^n$, which has the same dimension as the input $x$, and then adds a bias $b \\in \\mathbb{R}$, finally returning a scalar output $y \\in \\mathbb{R}$.</p> <p>Formally,</p> <p>$$ \\begin{align*} y = w_1 x_1 + w_2 x_2 + \\hspace{0.2cm} \\dots \\hspace{0.2cm} + w_n x_n + b &amp;= \\sum_{i = 1}^{n} w_i x_i + b  \\end{align*} $$</p> <p>The weight $w_i$ describes the importance of the corresponding feature $x_i$, indicating how much it contributes to computing the output.</p> <p>The weight vector $w$ and bias $b$ are called learnable parameters, meaning they are learned during the training process.</p> <p>Acually, we can add the bias $b$ in the weighted sum, by consedering the $w_0 = b$ and set the $x_0 = 1$.</p> <p>So,</p> <p>$$ \\begin{align*} y &amp;= \\sum_{i = 1}^{n} w_i x_i + b = \\sum_{i = 1}^{n} w_i x_i + w_0 x_0 = \\sum_{i = 0}^{n} w_i x_i  \\end{align*} $$</p> <p>There is another way to compute the output $y$ using the dot product of the input vector $x = (1, x_{org}) \\in \\mathbb{R}^{n+1}$ and the weight vector $w = (b, w_{org}) \\in \\mathbb{R}^{n+1}$ as follows:</p> <p>$$ y = w^T x $$</p> <p>\"In the previous section, we saw how a single artificial neuron operates. Now, we can map the same input vector $x \\in \\mathbb{R}^{n}$ to multiple neurons and perform the same operation as before. This creates a structure called a Fully Connected Layer, where all output nodes are fully connected to the input nodes.\"</p> Fully Connected Layer <p>Note</p> <p> We will adopt a notation to maintain consistency in writing equations where the weight connecting input node $i$ to output node $j$ is denoted as $w_{ij}$. </p> <p>Let's start with the first output, considering it as a single neuron performing the same computation as before.</p> <p>$$     y_{1} = w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + \\hspace{0.2cm} \\dots \\hspace{0.2cm} + w_{1n}x_n + w_{10} = \\sum_{i=1}^{n}w_{1i}x_i $$</p> <p>$$     y_{2} = w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + \\hspace{0.2cm} \\dots \\hspace{0.2cm} + w_{2n}x_n + w_{20} = \\sum_{i=1}^{n}w_{2i}x_i $$</p> <p>$$     y_{m} = w_{m1}x_1 + w_{m2}x_2 + w_{m3}x_3 + \\hspace{0.2cm} \\dots \\hspace{0.2cm} + w_{mn}x_n + w_{m0} = \\sum_{i=1}^{n}w_{mi}x_i $$</p> <p>Beautiful. Let's stack all the equations into a single system of linear equations.</p> <p>$$ \\begin{equation*} \\begin{cases}      &amp;y_{1} = w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + \\dots + w_{1n}x_n + w_{10} \\\\      &amp;y_{2} = w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + \\dots + w_{2n}x_n + w_{20} \\\\      &amp;\\vdots \\\\      &amp;y_{m} = w_{m1}x_1 + w_{m2}x_2 + w_{m3}x_3 + \\dots + w_{mn}x_n + w_{m0} \\\\ \\end{cases} \\end{equation*} $$</p> <p>Hey, does this remind you of something, a pattern here?</p> <p>Let's turn this system of linear equations into matrix multiplications.</p> <p>$$ \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix} = \\begin{bmatrix} w_{10} &amp; w_{11} &amp; w_{12} &amp; \\dots &amp; w_{1n} \\\\ w_{20} &amp; w_{21} &amp; w_{22} &amp; \\dots &amp; w_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ w_{m0} &amp; w_{m1} &amp; w_{m2} &amp; \\dots &amp; w_{mn} \\end{bmatrix} \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} $$</p> <p>Thus, we can use the matrix formula to describe the computation of a fully connected layer as:</p> <p>$$ \\begin{align*} \\mathbf{y} &amp;= W \\mathbf{x} \\end{align*} $$</p> <p>Where</p> <p>$$ W  = \\begin{bmatrix} w_{10} &amp; w_{11} &amp; w_{12} &amp; \\dots &amp; w_{1n} \\\\ w_{20} &amp; w_{21} &amp; w_{22} &amp; \\dots &amp; w_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ w_{m0} &amp; w_{m1} &amp; w_{m2} &amp; \\dots &amp; w_{mn} \\end{bmatrix} \\in \\mathbb{R}^{m \\times (n+1)} $$</p> <p>Here, $\\mathbf{x} \\in \\mathbb{R}^{(n+1)}$ and $\\mathbf{y} \\in \\mathbb{R}^{m}$ denote the input and output vectors of the fully connected layer, respectively.</p> <p>In the previous section, we demonstrated that we could construct a fully connected layer with any number of inputs, denoted as <code>in_features</code>, and produce any number of outputs, denoted as <code>out_features</code>, by constructing a learnable matrix $W$ with dimensions in_features $\\times$ out_features.</p> <p>The forward pass is performed when we compute the output, given a input vector $x \\in \\mathbb{R}^{(\\text{in\\_features} + 1)}$ :</p> <p>$$     \\mathbf{y} = W \\mathbf{x} $$</p> <p>This is the most exciting part.</p> <p>The whole point of machine learning is to train algorithms. The process of training involves evaluating a loss function (depending on the specific task), computing the gradients of this loss with respect to the model's parameters $W$, and then using any optimization methods, such as Adam, to train the model.</p> <p>Let's denote the loss function used to evaluate the model's performance as $L$.</p> <p>The following figure shows that the fully connected layer receives the gradient flows from the subsequent layer, denoted as $\\frac{\\partial L}{\\partial \\mathbf{y}}$. This quantity is used to compute the gradient of the loss with respect to the current layer's parameters $\\frac{\\partial L}{\\partial W}$. Then, it passes the gradient with respect to the input to the previous layers $\\frac{\\partial L}{\\partial \\mathbf{x}}$, following the chain rule in backpropagation.</p> <p>For instance, let's break down each derivative.</p> <p>The loss function is a scalar value, i.e., $L \\in \\mathbb{R}$. Let $\\mathbf{v}$ be a vector of n-dimensions, i.e., $\\mathbf{v} \\in \\mathbb{R}^n$.</p> <p>So, the derivative of $L$ with respect to $\\mathbf{v}$ is defined as the derivative of $L$ for each component of $\\mathbf{v}$. Formally:</p> <p>$$ \\frac{\\partial L}{\\partial \\mathbf{v}} = \\begin{bmatrix} \\frac{\\partial L}{\\partial v_1} \\\\ \\frac{\\partial L}{\\partial v_2} \\\\ \\vdots \\\\ \\frac{\\partial L}{\\partial v_n} \\end{bmatrix} $$</p> <p>With the same logic, given a matrix $M \\in \\mathbb{R}^{m \\times n}$, the derivative of $L$ with respect to $M$ is defined as the derivative of $L$ for each component of $M$. Formally:</p> <p>$$ \\frac{\\partial L}{\\partial M} = \\begin{bmatrix} \\frac{\\partial L}{\\partial M_{11}} &amp; \\frac{\\partial L}{\\partial M_{12}} &amp; \\cdots &amp; \\frac{\\partial L}{\\partial M_{1n}} \\\\ \\frac{\\partial L}{\\partial M_{21}} &amp; \\frac{\\partial L}{\\partial M_{22}} &amp; \\cdots &amp; \\frac{\\partial L}{\\partial M_{2n}} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial L}{\\partial M_{m1}} &amp; \\frac{\\partial L}{\\partial M_{m2}} &amp; \\cdots &amp; \\frac{\\partial L}{\\partial M_{mn}} \\end{bmatrix} $$</p> <p>Since our layer receives the quantity $\\frac{\\partial L}{\\partial \\mathbf{y}}$ during back-propagation, our task is to use it to compute the derivative of $L$ with respect to $W$.</p> <p>Given row index $i \\in \\{1, ..., n\\}$ and column index $j \\in \\{1, ..., m\\}$:</p> <p>$$ \\frac{\\partial L}{\\partial W_{ij}} = \\frac{\\partial L}{\\partial y_1} \\underbrace{\\frac{\\partial y_1}{\\partial W_{ij}}}_{=0} + \\frac{\\partial L}{\\partial y_2} \\underbrace{\\frac{\\partial y_2}{\\partial W_{ij}}}_{=0} + \\dots + \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial W_{ij}} + \\dots + \\frac{\\partial L}{\\partial y_n} \\underbrace{\\frac{\\partial y_n}{\\partial W_{ij}}}_{=0} $$</p> <p>Thus,</p> <p>$$ \\frac{\\partial L}{\\partial W_{ij}} = \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial W_{ij}} $$</p> <p>We have:</p> <p>$$ y_i = W_{i1}x_1 + \\dots + W_{ij}x_j + \\dots + W_{im}x_m $$</p> <p>Then, the derivative of $y_i$ with respect to $W_{ij}$ is:</p> <p>$$ \\frac{\\partial y_i}{\\partial W_{ij}} = x_j $$</p> <p>Finally,</p> <p>$$     \\forall i \\in  \\{1, ..., n \\}, j \\in \\{1, ..., m\\} \\mid \\frac{\\partial L}{\\partial W_{ij}} = \\frac{\\partial L}{\\partial y_i} x_j $$</p> <p>Using this formula to fill the matrix $\\frac{\\partial L}{\\partial W}$:</p> <p>$$ \\frac{\\partial L}{\\partial W} = \\begin{bmatrix} \\frac{\\partial L}{\\partial W_{11}} &amp; \\frac{\\partial L}{\\partial W_{12}} &amp; \\cdots &amp; \\frac{\\partial L}{\\partial W_{1n}} \\\\ \\frac{\\partial L}{\\partial W_{21}} &amp; \\frac{\\partial L}{\\partial W_{22}} &amp; \\cdots &amp; \\frac{\\partial L}{\\partial W_{2n}} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial L}{\\partial W_{m1}} &amp; \\frac{\\partial L}{\\partial W_{m2}} &amp; \\cdots &amp; \\frac{\\partial L}{\\partial W_{mn}} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial L}{\\partial y_1} x_1 &amp; \\frac{\\partial L}{\\partial y_1} x_2 &amp; \\cdots &amp;\\frac{\\partial L}{\\partial y_1} x_n \\\\ \\frac{\\partial L}{\\partial y_2} x_1 &amp; \\frac{\\partial L}{\\partial y_2} x_2 &amp; \\cdots &amp;\\frac{\\partial L}{\\partial y_2} x_n \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial L}{\\partial y_m} x_1 &amp; \\frac{\\partial L}{\\partial y_m} x_2 &amp; \\cdots &amp;\\frac{\\partial L}{\\partial y_m} x_n \\\\ \\end{bmatrix}  =  \\begin{bmatrix}     \\frac{\\partial L}{\\partial y_1} \\\\     \\frac{\\partial L}{\\partial y_2} \\\\     \\vdots \\\\     \\frac{\\partial L}{\\partial y_m} \\end{bmatrix} \\begin{bmatrix}     x_1 &amp; x_2 &amp; \\dots &amp; x_n \\\\ \\end{bmatrix} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{x}^T $$</p> <p>Finally,</p> <p>$$     \\frac{\\partial L}{\\partial W} =  \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{x}^T $$</p> <p>With the same logic as before, let's compute the derivative of $L$ with respect to input vector $\\mathbf{x}, i.e. $$\\frac{\\partial L}{\\partial \\mathbf{x}}$.</p> <p>For given $i \\in {1, ..., n}$</p> <p>$$ \\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial y_1}  \\underbrace{\\frac{\\partial y_1}{\\partial x_i}}_{W_{j1}} + \\dots + \\frac{\\partial L}{\\partial y_j}  \\underbrace{\\frac{\\partial y_j}{\\partial x_i}}_{W_{ji}} + \\dots + \\frac{\\partial L}{\\partial y_m}  \\underbrace{\\frac{\\partial y_m}{\\partial x_i}}_{W_{jm}} $$</p> <p>Because we have:</p> <p>$$ y_j = W_{j1}x_1 + \\dots + W_{ji}x_i + \\dots + W_{jm}x_m $$</p> <p>Thus,</p> <p>$$ \\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial y_1}W_{j1} + \\dots + \\frac{\\partial L}{\\partial y_j}W_{ji} + \\dots + \\frac{\\partial L}{\\partial y_m} W_{jm} $$</p> <p>Using this formula to fill the vector $\\frac{\\partial L}{\\partial \\mathbf{x}}$:</p> <p>$$ \\frac{\\partial L}{\\partial \\mathbf{x}} = \\begin{bmatrix}     \\frac{\\partial L}{\\partial x_1} \\\\     \\frac{\\partial L}{\\partial x_2} \\\\     \\vdots \\\\     \\frac{\\partial L}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix}     \\frac{\\partial L}{\\partial y_1}W_{11} + \\dots + \\frac{\\partial L}{\\partial y_j}W_{j1} + \\dots + \\frac{\\partial L}{\\partial y_m} W_{m1} \\\\     \\frac{\\partial L}{\\partial y_1}W_{12} + \\dots + \\frac{\\partial L}{\\partial y_j}W_{j2} + \\dots + \\frac{\\partial L}{\\partial y_m} W_{m2} \\\\     \\vdots \\\\     \\frac{\\partial L}{\\partial y_1}W_{1n} + \\dots + \\frac{\\partial L}{\\partial y_j}W_{jn} + \\dots + \\frac{\\partial L}{\\partial y_m} W_{mn} \\end{bmatrix} = \\begin{bmatrix}     W_{11} &amp; W_{21} &amp; \\dots &amp; W_{m1} \\\\     W_{12} &amp; W_{22} &amp; \\dots &amp; W_{m2} \\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     W_{1n} &amp; W_{2n} &amp; \\dots &amp; W_{mn} \\end{bmatrix} \\begin{bmatrix}     \\frac{\\partial L}{\\partial y_1} \\\\     \\frac{\\partial L}{\\partial y_2} \\\\     \\vdots \\\\     \\frac{\\partial L}{\\partial y_m} \\end{bmatrix} =  W^T \\frac{\\partial L}{\\partial \\mathbf{y}} $$</p> <p>Finally,</p> <p>$$ \\frac{\\partial L}{\\partial \\mathbf{x}} = W^T \\frac{\\partial L}{\\partial \\mathbf{y}} $$</p> <p>Rules to Compute Gradients</p> <p>The layer receives the gradient flow $\\frac{\\partial L}{\\partial \\mathbf{y}}$. Therefore, the gradients can be computed as follows:</p> <p> $$     \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{x}^T $$ </p> <p> $$     \\frac{\\partial L}{\\partial \\mathbf{x}} = W^T \\frac{\\partial L}{\\partial \\mathbf{y}} $$ </p> <p>At this stage we've covered all we need to implement the fully connected layer using only Numpy.</p> <p>For instance, we have two pass modes. First, the forward pass performers of computing the output. Second, the backward pass where the gradients are calculated, helps us update the model's parameters, to make more accurate predictions.</p> In\u00a0[\u00a0]: Copied!"},{"location":"docs/nanotorch-linear-layer/#nnmoduleslinearlinear","title":"<code>nn.modules.linear.Linear</code>\u00b6","text":""},{"location":"docs/nanotorch-linear-layer/#01-theory-build-fully-connected-layer-from-scratch","title":"01. Theory - Build Fully Connected Layer from scratch\u00b6","text":""},{"location":"docs/nanotorch-linear-layer/#11-neural-nets-artificial-neuron","title":"1.1 Neural Nets - Artificial Neuron\u00b6","text":""},{"location":"docs/nanotorch-linear-layer/#12-fully-connected-layer","title":"1.2 Fully Connected Layer\u00b6","text":""},{"location":"docs/nanotorch-linear-layer/#14-forward-propopagation","title":"1.4 Forward Propopagation\u00b6","text":""},{"location":"docs/nanotorch-linear-layer/#15-back-propagation","title":"1.5 Back-Propagation\u00b6","text":""},{"location":"docs/nanotorch-linear-layer/#151-compute-fracpartial-lpartial-w","title":"1.5.1 Compute $\\frac{\\partial L}{\\partial W}$\u00b6","text":""},{"location":"docs/nanotorch-linear-layer/#152-compute-fracpartial-lpartial-mathbfx","title":"1.5.2 Compute $\\frac{\\partial L}{\\partial \\mathbf{x}}$\u00b6","text":""},{"location":"docs/nanotorch-linear-layer/#02-implementation-build-fully-connected-layer-from-scratch","title":"02. Implementation - Build Fully Connected Layer from scratch\u00b6","text":""},{"location":"docs/tensor/","title":"Tensor","text":""},{"location":"docs/tensor/#whats-tensor","title":"What's Tensor?","text":"<p>Deep learning at a low level can be seen as just tensor manipulation. A tensor can be defined as a generalization of vectors and matrices to higher dimensions. Thus, tensors are at the heart of deep learning.</p>  Tensor  <p>The first thing we need to address in building <code>NanoTorch</code> is having a powerful and efficient tensors module that ensures numerical stability. Achieving this can be challenging and is not our primary mission here. Therefore, we're going to use <code>Numpy</code>, a Python library that provides powerful N-dimensional arrays. <code>Numpy</code> is fast (C implementation) and easy to use, making it an excellent choice for handling tensor operations.</p> <p>However, we need to make some decisions about the way of integrating the <code>Numpy</code> library to ensure compatibility with other modules in NanoTorch. We'll discuss these decisions in a few moments.</p>"},{"location":"docs/tensor/#numpy-for-numerical-operations","title":"Numpy for Numerical Operations","text":"<p>Numpy is a library based on <code>ndarray</code>, a multi-dimensional array of the same type. It offers algebraic operations in an efficient way. Let's look at some examples with Numpy and then explore how we can use it to build the <code>tensor</code> module for <code>NanoTorch</code>.</p> <pre><code>import numpy as np\n</code></pre> <pre><code>\u251c\u2500\u2500 nanotorch\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tensor\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ops.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tensor.py\n</code></pre>"},{"location":"docs/tensor/tensor/","title":"Tensor","text":"<p>Deep learning at a low level can be seen as just tensor manipulation. A tensor can be defined as a generalization of vectors and matrices to higher dimensions. Thus, tensors are at the heart of deep learning.</p>  Tensor  <p>The first thing we need to address in building <code>NanoTorch</code> is having a powerful and efficient tensors module that ensures numerical stability. Achieving this can be challenging and is not our primary mission here. Therefore, we're going to use <code>Numpy</code>, a Python library that provides powerful N-dimensional arrays. <code>Numpy</code> is fast (C implementation) and easy to use, making it an excellent choice for handling tensor operations.</p> <p>However, we need to make some decisions about the way of integrating the <code>Numpy</code> library to ensure compatibility with other modules in NanoTorch. We'll discuss these decisions in a few moments.</p> <p>The tensor module contains two files, <code>tensor.py</code> that include implementing the <code>Tensor</code> class. The <code>ops.py</code> contains functions and operations such as <code>dot</code>, <code>sum</code>, etc.</p> <pre><code>\u251c\u2500\u2500 nanotorch\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tensor\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ops.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tensor.py\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>import numpy as np \nfrom typing import Self\n</pre> import numpy as np  from typing import Self In\u00a0[\u00a0]: Copied! <pre>class Tensor(np.ndarray):\n\n    def __new__(cls, input: Self) -&gt; Self:\n        \n        if isinstance(input, (int, float)): input = np.array([input,])\n        \n        if not isinstance(input, (Tensor, list, tuple, np.ndarray)): \n            raise ValueError(f\"the 'input' attributes must be list, tuple, numpy.ndarray. But '{input.__class__.__name__}' is given\") \n        \n        # reshape to 2-d if the input is 1-d:\n        if not isinstance(input, np.ndarray): input = np.array(input)\n        if input.ndim == 1: input = input.reshape(1, -1)\n        \n        # create a view : \n        obj = np.asanyarray(input).view(cls)\n\n        return obj\n</pre> class Tensor(np.ndarray):      def __new__(cls, input: Self) -&gt; Self:                  if isinstance(input, (int, float)): input = np.array([input,])                  if not isinstance(input, (Tensor, list, tuple, np.ndarray)):              raise ValueError(f\"the 'input' attributes must be list, tuple, numpy.ndarray. But '{input.__class__.__name__}' is given\")                   # reshape to 2-d if the input is 1-d:         if not isinstance(input, np.ndarray): input = np.array(input)         if input.ndim == 1: input = input.reshape(1, -1)                  # create a view :          obj = np.asanyarray(input).view(cls)          return obj"},{"location":"docs/tensor/tensor/#tensor","title":"Tensor\u00b6","text":""},{"location":"docs/tensor/tensor/#whats-tensor","title":"What's Tensor?\u00b6","text":""},{"location":"docs/tensor/tensor/#the-structure-of-tensor-module","title":"The Structure of <code>Tensor</code> Module\u00b6","text":""},{"location":"docs/tensor/tensor/#tensor-class","title":"<code>Tensor</code> class\u00b6","text":""}]}